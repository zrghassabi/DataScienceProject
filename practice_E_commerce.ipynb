{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3RVSzJdRKuFAEbXFFdrDZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrghassabi/DataScienceProject/blob/main/practice_E_commerce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM-based web traffic forecasting model"
      ],
      "metadata": {
        "id": "ibOYk3b2Q8Lp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsUicyPG8xPR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# **Step 1: Simulated Web Traffic Data**\n",
        "np.random.seed(42)\n",
        "date_range = pd.date_range(start=\"2021-01-01\", periods=36, freq=\"M\")\n",
        "web_traffic_data = pd.DataFrame({\n",
        "    'Date': date_range,\n",
        "    'Monthly_Visitors': np.random.randint(500000, 1500000, len(date_range)),\n",
        "    'Ad_Spend': np.random.uniform(50000, 200000, len(date_range)),\n",
        "    'SEO_Ranking': np.random.randint(1, 50, len(date_range)),\n",
        "    'Competitor_Issues': np.random.choice([0, 1], len(date_range), p=[0.8, 0.2]),\n",
        "    'Seasonality': np.sin(np.linspace(0, 2 * np.pi, len(date_range)))\n",
        "})\n",
        "\n",
        "# **Step 2: Preprocessing**\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(web_traffic_data[['Monthly_Visitors', 'Ad_Spend', 'SEO_Ranking', 'Competitor_Issues', 'Seasonality']])\n",
        "\n",
        "# **Step 3: Prepare Data for LSTM**\n",
        "def create_sequences(data, seq_length=12):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 12\n",
        "X_lstm, y_lstm = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# **Step 4: Define LSTM Model**\n",
        "class WebTrafficLSTM(nn.Module):\n",
        "    def __init__(self, input_size=5, hidden_size=50, num_layers=2):\n",
        "        super(WebTrafficLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        return self.fc(lstm_out[:, -1, :])\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "lstm_model = WebTrafficLSTM()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# **Step 5: Train the LSTM Model**\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = lstm_model(X_train_torch)\n",
        "    loss = criterion(y_pred, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# **Step 6: Forecast Next 6 Months**\n",
        "future_dates = pd.date_range(start=web_traffic_data['Date'].max() + pd.DateOffset(months=1), periods=6, freq=\"M\")\n",
        "future_data = np.random.rand(6, seq_length, 5)  # Generate random future input data\n",
        "future_data_torch = torch.tensor(future_data, dtype=torch.float32)\n",
        "future_predictions = lstm_model(future_data_torch).detach().numpy()\n",
        "\n",
        "# Rescale predictions\n",
        "future_predictions_rescaled = scaler.inverse_transform(np.concatenate((future_predictions, np.zeros((6, 4))), axis=1))[:, 0]\n",
        "\n",
        "# **Step 7: Display Forecasted Traffic Trends**\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(web_traffic_data['Date'], web_traffic_data['Monthly_Visitors'], label=\"Historical Traffic\")\n",
        "plt.plot(future_dates, future_predictions_rescaled, marker='o', linestyle=\"--\", label=\"Predicted Traffic (LSTM)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Monthly Visitors\")\n",
        "plt.title(\"LSTM-Based Web Traffic Forecasting\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# **Output Forecasted Values**\n",
        "future_forecast_df = pd.DataFrame({'Date': future_dates, 'Predicted_Monthly_Visitors': future_predictions_rescaled})\n",
        "print(future_forecast_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(date_range, len(date_range),len(data_scaled))"
      ],
      "metadata": {
        "id": "CyWXh1qFF0EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(web_traffic_data.head())"
      ],
      "metadata": {
        "id": "RSRBBFzw-2d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_scaled[:12]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "46B4P0eN_MfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=data_scaled[0:0+12]\n",
        "y=data_scaled[0:0+12,0]\n",
        "\n",
        "print(x,y)"
      ],
      "metadata": {
        "id": "pCFqzwRXANPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_lstm.shape, y_lstm.shape\n",
        "\n",
        "print(\"X_lstm (first 3 rows):\\n\", X_lstm[:5])\n",
        "print(\"\\ny_lstm (first 3 rows):\\n\", y_lstm[:5])\n"
      ],
      "metadata": {
        "id": "8RZI5pdX-F0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimize LSTM"
      ],
      "metadata": {
        "id": "4Dy_FKz1RHG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Step 4: Optimized LSTM Model**\n",
        "class WebTrafficLSTM(nn.Module):\n",
        "    def __init__(self, input_size=5, hidden_size=100, num_layers=3, dropout=0.2):\n",
        "        super(WebTrafficLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        return self.fc(lstm_out[:, -1, :])\n",
        "\n",
        "# Initialize model with optimized hyperparameters\n",
        "lstm_model = WebTrafficLSTM()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "\n",
        "# **Step 5: Train the Optimized LSTM Model**\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = lstm_model(X_train_torch)\n",
        "    loss = criterion(y_pred, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# **Step 6: Forecast Next 6 Months with Optimized Model**\n",
        "future_dates = pd.date_range(start=web_traffic_data['Date'].max() + pd.DateOffset(months=1), periods=6, freq=\"M\")\n",
        "future_data = np.random.rand(6, seq_length, 5)  # Generate random future input data\n",
        "future_data_torch = torch.tensor(future_data, dtype=torch.float32)\n",
        "future_predictions = lstm_model(future_data_torch).detach().numpy()\n",
        "\n",
        "# Rescale predictions\n",
        "future_predictions_rescaled = scaler.inverse_transform(np.concatenate((future_predictions, np.zeros((6, 4))), axis=1))[:, 0]\n",
        "\n",
        "# **Step 7: Display Forecasted Traffic Trends**\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(web_traffic_data['Date'], web_traffic_data['Monthly_Visitors'], label=\"Historical Traffic\")\n",
        "plt.plot(future_dates, future_predictions_rescaled, marker='o', linestyle=\"--\", label=\"Predicted Traffic (Optimized LSTM)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Monthly Visitors\")\n",
        "plt.title(\"Optimized LSTM-Based Web Traffic Forecasting\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# **Output Forecasted Values**\n",
        "future_forecast_df = pd.DataFrame({'Date': future_dates, 'Predicted_Monthly_Visitors': future_predictions_rescaled})\n",
        "print(future_forecast_df)\n"
      ],
      "metadata": {
        "id": "PmUvVut8UNPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# **Step 1: Simulated Web Traffic Data**\n",
        "np.random.seed(42)\n",
        "date_range = pd.date_range(start=\"2021-01-01\", periods=36, freq=\"M\")\n",
        "web_traffic_data = pd.DataFrame({\n",
        "    'Date': date_range,\n",
        "    'Monthly_Visitors': np.random.randint(500000, 1500000, len(date_range)),\n",
        "    'Ad_Spend': np.random.uniform(50000, 200000, len(date_range)),\n",
        "    'SEO_Ranking': np.random.randint(1, 50, len(date_range)),\n",
        "    'Competitor_Issues': np.random.choice([0, 1], len(date_range), p=[0.8, 0.2]),\n",
        "    'Seasonality': np.sin(np.linspace(0, 2 * np.pi, len(date_range)))\n",
        "})\n",
        "\n",
        "# **Step 2: Preprocessing**\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(web_traffic_data[['Monthly_Visitors', 'Ad_Spend', 'SEO_Ranking', 'Competitor_Issues', 'Seasonality']])\n",
        "\n",
        "# **Step 3: Prepare Data for Transformer Model**\n",
        "def create_sequences(data, seq_length=12):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 12\n",
        "X_lstm, y_lstm = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# **Step 4: Transformer-Based Forecasting Model**\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size=5, d_model=64, nhead=4, num_layers=3, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.encoder = nn.Linear(input_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.transformer(x, x)\n",
        "        return self.decoder(x[:, -1, :])\n",
        "\n",
        "# Initialize Transformer Model\n",
        "transformer_model = TransformerModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(transformer_model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "\n",
        "# **Step 5: Train Transformer Model**\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = transformer_model(X_train_torch)\n",
        "    loss = criterion(y_pred, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# **Step 6: Visualize Attention Weights**\n",
        "attention_weights = torch.softmax(transformer_model.transformer.encoder.layers[0].self_attn.in_proj_weight, dim=1).detach().numpy()\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(attention_weights, cmap='viridis', aspect='auto')\n",
        "plt.colorbar(label='Attention Weight')\n",
        "plt.xlabel('Input Sequence Position')\n",
        "plt.ylabel('Attention Head')\n",
        "plt.title('Visualization of Attention Weights in Transformer Model')\n",
        "plt.show()\n",
        "\n",
        "# **Step 7: Forecast Next 6 Months with Transformer Model**\n",
        "future_dates = pd.date_range(start=web_traffic_data['Date'].max() + pd.DateOffset(months=1), periods=6, freq=\"M\")\n",
        "future_data = np.random.rand(6, seq_length, 5)  # Generate random future input data\n",
        "future_data_torch = torch.tensor(future_data, dtype=torch.float32)\n",
        "future_predictions = transformer_model(future_data_torch).detach().numpy()\n",
        "\n",
        "# Rescale predictions\n",
        "future_predictions_rescaled = scaler.inverse_transform(np.concatenate((future_predictions, np.zeros((6, 4))), axis=1))[:, 0]\n",
        "\n",
        "# **Step 8: Display Forecasted Traffic Trends**\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(web_traffic_data['Date'], web_traffic_data['Monthly_Visitors'], label=\"Historical Traffic\")\n",
        "plt.plot(future_dates, future_predictions_rescaled, marker='o', linestyle=\"--\", label=\"Predicted Traffic (Transformer Model)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Monthly Visitors\")\n",
        "plt.title(\"Transformer-Based Web Traffic Forecasting with Attention\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# **Output Forecasted Values**\n",
        "future_forecast_df = pd.DataFrame({'Date': future_dates, 'Predicted_Monthly_Visitors': future_predictions_rescaled})\n",
        "print(future_forecast_df)\n"
      ],
      "metadata": {
        "id": "kXhlVmu1Vmbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# **Step 1: Simulated Web Traffic Data**\n",
        "np.random.seed(42)\n",
        "date_range = pd.date_range(start=\"2021-01-01\", periods=36, freq=\"M\")\n",
        "web_traffic_data = pd.DataFrame({\n",
        "    'Date': date_range,\n",
        "    'Monthly_Visitors': np.random.randint(500000, 1500000, len(date_range)),\n",
        "    'Ad_Spend': np.random.uniform(50000, 200000, len(date_range)),\n",
        "    'SEO_Ranking': np.random.randint(1, 50, len(date_range)),\n",
        "    'Competitor_Issues': np.random.choice([0, 1], len(date_range), p=[0.8, 0.2]),\n",
        "    'Seasonality': np.sin(np.linspace(0, 2 * np.pi, len(date_range)))\n",
        "})\n",
        "\n",
        "# **Step 2: Preprocessing**\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(web_traffic_data[['Monthly_Visitors', 'Ad_Spend', 'SEO_Ranking', 'Competitor_Issues', 'Seasonality']])\n",
        "\n",
        "# **Step 3: Prepare Data for Transformer Model**\n",
        "def create_sequences(data, seq_length=12):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 12\n",
        "X_transformer, y_transformer = create_sequences(data_scaled, seq_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_transformer, y_transformer, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# **Step 4: Informer-Based Forecasting Model**\n",
        "class InformerModel(nn.Module):\n",
        "    def __init__(self, input_size=5, d_model=64, nhead=4, num_layers=3, dropout=0.1):\n",
        "        super(InformerModel, self).__init__()\n",
        "        self.encoder = nn.Linear(input_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.transformer(x, x)\n",
        "        return self.decoder(x[:, -1, :])\n",
        "\n",
        "# Initialize Informer Model\n",
        "informer_model = InformerModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(informer_model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "\n",
        "# **Step 5: Train Informer Model**\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = informer_model(X_train_torch)\n",
        "    loss = criterion(y_pred, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# **Step 6: Visualize Attention Weights**\n",
        "attention_weights = torch.softmax(informer_model.transformer.encoder.layers[0].self_attn.in_proj_weight, dim=1).detach().numpy()\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(attention_weights, cmap='viridis', aspect='auto')\n",
        "plt.colorbar(label='Attention Weight')\n",
        "plt.xlabel('Input Sequence Position')\n",
        "plt.ylabel('Attention Head')\n",
        "plt.title('Visualization of Attention Weights in Informer Model')\n",
        "plt.show()\n",
        "\n",
        "# **Step 7: Forecast Next 6 Months with Informer Model**\n",
        "future_dates = pd.date_range(start=web_traffic_data['Date'].max() + pd.DateOffset(months=1), periods=6, freq=\"M\")\n",
        "future_data = np.random.rand(6, seq_length, 5)  # Generate random future input data\n",
        "future_data_torch = torch.tensor(future_data, dtype=torch.float32)\n",
        "future_predictions = informer_model(future_data_torch).detach().numpy()\n",
        "\n",
        "# Rescale predictions\n",
        "future_predictions_rescaled = scaler.inverse_transform(np.concatenate((future_predictions, np.zeros((6, 4))), axis=1))[:, 0]\n",
        "\n",
        "# **Step 8: Display Forecasted Traffic Trends**\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(web_traffic_data['Date'], web_traffic_data['Monthly_Visitors'], label=\"Historical Traffic\")\n",
        "plt.plot(future_dates, future_predictions_rescaled, marker='o', linestyle=\"--\", label=\"Predicted Traffic (Informer Model)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Monthly Visitors\")\n",
        "plt.title(\"Informer-Based Web Traffic Forecasting with Attention\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# **Output Forecasted Values**\n",
        "future_forecast_df = pd.DataFrame({'Date': future_dates, 'Predicted_Monthly_Visitors': future_predictions_rescaled})\n",
        "print(future_forecast_df)\n"
      ],
      "metadata": {
        "id": "DsqOG5TJXqay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "telecom churn prediction"
      ],
      "metadata": {
        "id": "Z6KQGzWLWRKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# **Telecom Customer Churn Prediction Model with LSTM**\n",
        "\n",
        "# Generate simulated telecom customer data\n",
        "np.random.seed(42)\n",
        "num_customers = 5000\n",
        "\n",
        "telecom_data = pd.DataFrame({\n",
        "    'Customer_ID': np.arange(1, num_customers + 1),\n",
        "    'Tenure_Months': np.random.randint(1, 72, num_customers),\n",
        "    'Monthly_Bill': np.random.uniform(20, 150, num_customers),\n",
        "    'Total_Usage_GB': np.random.uniform(5, 100, num_customers),\n",
        "    'Customer_Support_Calls': np.random.randint(0, 10, num_customers),\n",
        "    'Contract_Type': np.random.choice([0, 1], num_customers, p=[0.6, 0.4]),\n",
        "    'Payment_Method': np.random.choice([0, 1, 2, 3], num_customers),\n",
        "    'Churn': np.random.choice([0, 1], num_customers, p=[0.75, 0.25])\n",
        "})\n",
        "\n",
        "# **Step 1: Preprocessing & Feature Engineering**\n",
        "X = telecom_data.drop(columns=['Customer_ID', 'Churn'])\n",
        "y = telecom_data['Churn']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Reshape for LSTM (batch_size, seq_length, num_features)\n",
        "X_train_torch = X_train_torch.view(X_train_torch.shape[0], 1, X_train_torch.shape[1])\n",
        "X_test_torch = X_test_torch.view(X_test_torch.shape[0], 1, X_test_torch.shape[1])\n",
        "\n",
        "# **Step 2: Define LSTM Model**\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=50, num_layers=2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        return torch.sigmoid(self.fc(lstm_out[:, -1, :]))\n",
        "\n",
        "# Initialize and train the model\n",
        "input_size = X_train.shape[1]\n",
        "lstm_model = LSTMModel(input_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "# **Step 3: Train LSTM Model**\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = lstm_model(X_train_torch)\n",
        "    loss = criterion(y_pred, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# **Step 4: Evaluate Model**\n",
        "y_pred_test = lstm_model(X_test_torch).detach().numpy()\n",
        "y_pred_test = (y_pred_test > 0.5).astype(int)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = np.mean(y_pred_test.flatten() == y_test.values)\n",
        "print(f'LSTM Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# **Step 5: Predict Churn for a New Customer**\n",
        "def predict_churn_lstm(customer_data_point):\n",
        "    customer_scaled = scaler.transform([customer_data_point])\n",
        "    customer_torch = torch.tensor(customer_scaled, dtype=torch.float32).view(1, 1, -1)\n",
        "    prediction = lstm_model(customer_torch).item()\n",
        "    return \"Churn\" if prediction > 0.5 else \"Retained\"\n",
        "\n",
        "# Example customer: 24-month tenure, $75 bill, 50GB data usage, 2 support calls, annual contract, PayPal payment\n",
        "sample_customer = [24, 75, 50, 2, 1, 2]\n",
        "churn_decision = predict_churn_lstm(sample_customer)\n",
        "print(f\"Churn Prediction for Sample Customer: {churn_decision}\")\n"
      ],
      "metadata": {
        "id": "r2ezJ_SHWTQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# **Telecom Customer Churn Prediction Model with Transformer**\n",
        "\n",
        "# Generate simulated telecom customer data\n",
        "np.random.seed(42)\n",
        "num_customers = 5000\n",
        "\n",
        "telecom_data = pd.DataFrame({\n",
        "    'Customer_ID': np.arange(1, num_customers + 1),\n",
        "    'Tenure_Months': np.random.randint(1, 72, num_customers),\n",
        "    'Monthly_Bill': np.random.uniform(20, 150, num_customers),\n",
        "    'Total_Usage_GB': np.random.uniform(5, 100, num_customers),\n",
        "    'Customer_Support_Calls': np.random.randint(0, 10, num_customers),\n",
        "    'Contract_Type': np.random.choice([0, 1], num_customers, p=[0.6, 0.4]),\n",
        "    'Payment_Method': np.random.choice([0, 1, 2, 3], num_customers),\n",
        "    'Churn': np.random.choice([0, 1], num_customers, p=[0.75, 0.25])\n",
        "})\n",
        "\n",
        "# **Step 1: Preprocessing & Feature Engineering**\n",
        "X = telecom_data.drop(columns=['Customer_ID', 'Churn'])\n",
        "y = telecom_data['Churn']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Reshape for Transformer (batch_size, seq_length, num_features)\n",
        "X_train_torch = X_train_torch.view(X_train_torch.shape[0], 1, X_train_torch.shape[1])\n",
        "X_test_torch = X_test_torch.view(X_test_torch.shape[0], 1, X_test_torch.shape[1])\n",
        "\n",
        "# **Step 2: Define Transformer Model**\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, d_model=64, nhead=4, num_layers=3, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.encoder = nn.Linear(input_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.transformer(x, x)\n",
        "        return torch.sigmoid(self.decoder(x[:, -1, :]))\n",
        "\n",
        "# Initialize and train the model\n",
        "input_size = X_train.shape[1]\n",
        "transformer_model = TransformerModel(input_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "\n",
        "# **Step 3: Train Transformer Model**\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = transformer_model(X_train_torch)\n",
        "    loss = criterion(y_pred, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# **Step 4: Evaluate Model**\n",
        "y_pred_test = transformer_model(X_test_torch).detach().numpy()\n",
        "y_pred_test = (y_pred_test > 0.5).astype(int)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = np.mean(y_pred_test.flatten() == y_test.values)\n",
        "print(f'Transformer Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# **Step 5: Predict Churn for a New Customer**\n",
        "def predict_churn_transformer(customer_data_point):\n",
        "    customer_scaled = scaler.transform([customer_data_point])\n",
        "    customer_torch = torch.tensor(customer_scaled, dtype=torch.float32).view(1, 1, -1)\n",
        "    prediction = transformer_model(customer_torch).item()\n",
        "    return \"Churn\" if prediction > 0.5 else \"Retained\"\n",
        "\n",
        "# Example customer: 24-month tenure, $75 bill, 50GB data usage, 2 support calls, annual contract, PayPal payment\n",
        "sample_customer = [24, 75, 50, 2, 1, 2]\n",
        "churn_decision = predict_churn_transformer(sample_customer)\n",
        "print(f\"Churn Prediction for Sample Customer: {churn_decision}\")\n"
      ],
      "metadata": {
        "id": "YuYKWjvmZLvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# **Telecom Customer Churn Prediction Model with Optimized Transformer & API Deployment**\n",
        "\n",
        "# Generate simulated telecom customer data\n",
        "np.random.seed(42)\n",
        "num_customers = 5000\n",
        "\n",
        "telecom_data = pd.DataFrame({\n",
        "    'Customer_ID': np.arange(1, num_customers + 1),\n",
        "    'Tenure_Months': np.random.randint(1, 72, num_customers),\n",
        "    'Monthly_Bill': np.random.uniform(20, 150, num_customers),\n",
        "    'Total_Usage_GB': np.random.uniform(5, 100, num_customers),\n",
        "    'Customer_Support_Calls': np.random.randint(0, 10, num_customers),\n",
        "    'Contract_Type': np.random.choice([0, 1], num_customers, p=[0.6, 0.4]),\n",
        "    'Payment_Method': np.random.choice([0, 1, 2, 3], num_customers),\n",
        "    'Churn': np.random.choice([0, 1], num_customers, p=[0.75, 0.25])\n",
        "})\n",
        "\n",
        "# **Step 1: Preprocessing & Feature Engineering**\n",
        "X = telecom_data.drop(columns=['Customer_ID', 'Churn'])\n",
        "y = telecom_data['Churn']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Reshape for Transformer (batch_size, seq_length, num_features)\n",
        "X_train_torch = X_train_torch.view(X_train_torch.shape[0], 1, X_train_torch.shape[1])\n",
        "X_test_torch = X_test_torch.view(X_test_torch.shape[0], 1, X_test_torch.shape[1])\n",
        "\n",
        "# **Step 2: Optimized Transformer Model**\n",
        "class OptimizedTransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, d_model=128, nhead=8, num_layers=4, dropout=0.2):\n",
        "        super(OptimizedTransformerModel, self).__init__()\n",
        "        self.encoder = nn.Linear(input_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.transformer(x, x)\n",
        "        return torch.sigmoid(self.decoder(x[:, -1, :]))\n",
        "\n",
        "# Initialize Optimized Transformer Model\n",
        "input_size = X_train.shape[1]\n",
        "transformer_model = OptimizedTransformerModel(input_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.AdamW(transformer_model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "\n",
        "# **Step 3: Train Optimized Transformer Model**\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = transformer_model(X_train_torch)\n",
        "    loss = criterion(y_pred, y_train_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# **Step 4: Evaluate Model**\n",
        "y_pred_test = transformer_model(X_test_torch).detach().numpy()\n",
        "y_pred_test = (y_pred_test > 0.5).astype(int)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = np.mean(y_pred_test.flatten() == y_test.values)\n",
        "print(f'Optimized Transformer Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# **Step 5: API Deployment (Flask)**\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.json['customer_data']\n",
        "    customer_scaled = scaler.transform([data])\n",
        "    customer_torch = torch.tensor(customer_scaled, dtype=torch.float32).view(1, 1, -1)\n",
        "    prediction = transformer_model(customer_torch).item()\n",
        "    result = \"Churn\" if prediction > 0.5 else \"Retained\"\n",
        "    return jsonify({'prediction': result})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "mqVePgpMbn85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# **Step 1: Generate Simulated Supply Chain Data**\n",
        "np.random.seed(42)\n",
        "num_samples = 5000\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Warehouse_ID': np.random.randint(1, 50, num_samples),\n",
        "    'Product_Category': np.random.randint(1, 20, num_samples),\n",
        "    'Stock_Level': np.random.randint(10, 1000, num_samples),\n",
        "    'Lead_Time_Days': np.random.randint(1, 30, num_samples),\n",
        "    'Supplier_Reliability': np.random.uniform(0.7, 1.0, num_samples),\n",
        "    'Demand_Volatility': np.random.uniform(0.1, 0.9, num_samples),\n",
        "    'Shipping_Cost': np.random.uniform(5, 50, num_samples),\n",
        "    'Restocking_Quantity': np.random.randint(10, 500, num_samples)\n",
        "})\n",
        "\n",
        "# **Step 2: Preprocessing & Feature Engineering**\n",
        "X = data.drop(columns=['Restocking_Quantity'])\n",
        "y = data['Restocking_Quantity']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# **Step 3: Train Machine Learning Model (Random Forest Regressor)**\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "\n",
        "# **Step 4: Evaluate Model Performance**\n",
        "rf_mae = np.mean(np.abs(y_test - y_pred_rf))\n",
        "print(f'Mean Absolute Error (MAE): {rf_mae:.2f}')\n",
        "\n",
        "# **Step 5: Feature Importance Analysis**\n",
        "feature_importance_rf = rf_model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "# Plot Feature Importance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(feature_names, feature_importance_rf, color='skyblue')\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance in Supply Chain Optimization (Random Forest)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# **Step 6: Predict Restocking Needs for a New Warehouse Order**\n",
        "def predict_restocking(warehouse_data):\n",
        "    warehouse_scaled = scaler.transform([warehouse_data])\n",
        "    prediction = rf_model.predict(warehouse_scaled)\n",
        "    return int(prediction[0])\n",
        "\n",
        "# Example warehouse input: Warehouse 10, Category 5, 200 stock level, 15 days lead time, reliability 0.9, volatility 0.3, shipping cost 25\n",
        "sample_warehouse = [10, 5, 200, 15, 0.9, 0.3, 25]\n",
        "restocking_quantity = predict_restocking(sample_warehouse)\n",
        "print(f\"Predicted Restocking Quantity: {restocking_quantity}\")\n"
      ],
      "metadata": {
        "id": "OABCXOZaKOtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wayfair pricing case Study"
      ],
      "metadata": {
        "id": "YEbJqm2jNUw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Data Simulation (since we don't have real Wayfair data)\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'product_id': np.arange(1000),\n",
        "    'price': np.random.uniform(10, 500, 1000),\n",
        "    'competitor_price': np.random.uniform(8, 520, 1000),\n",
        "    'demand_index': np.random.uniform(0.5, 1.5, 1000),\n",
        "    'inventory_level': np.random.randint(50, 500, 1000),\n",
        "    'seasonality_index': np.random.uniform(0.8, 1.2, 1000)\n",
        "}\n",
        "\n",
        "# Generating sales based on features with some noise\n",
        "sales = (2000 / data['price']) * data['demand_index'] * data['seasonality_index'] + np.random.normal(0, 10, 1000)\n",
        "data['units_sold'] = sales.astype(int)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Exploratory Data Analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='price', y='units_sold', data=df)\n",
        "plt.title('Price vs Units Sold')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Model Development\n",
        "X = df[['price', 'competitor_price', 'demand_index', 'inventory_level', 'seasonality_index']]\n",
        "y = df['units_sold']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Baseline Model: Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Advanced Model: XGBoost\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Evaluation\n",
        "lr_preds = lr_model.predict(X_test)\n",
        "xgb_preds = xgb_model.predict(X_test)\n",
        "\n",
        "print(\"Linear Regression R^2:\", r2_score(y_test, lr_preds))\n",
        "print(\"XGBoost R^2:\", r2_score(y_test, xgb_preds))\n",
        "\n",
        "# Step 5: Visualization of Predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test.values, label='Actual Units Sold')\n",
        "plt.plot(lr_preds, label='Linear Regression Predictions')\n",
        "plt.plot(xgb_preds, label='XGBoost Predictions')\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Units Sold')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EJ_BQpH-NYB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# 1. Data Simulation\n",
        "np.random.seed(42)\n",
        "data = {\n",
        "    'product_id': np.arange(1000),\n",
        "    'base_price': np.random.uniform(10, 500, 1000),\n",
        "    'demand': np.random.randint(50, 200, 1000),\n",
        "    'competitor_price': np.random.uniform(8, 520, 1000),\n",
        "    'inventory_level': np.random.randint(10, 300, 1000),\n",
        "    'seasonality': np.random.choice([0, 1], 1000),\n",
        "    'sales': np.random.randint(30, 250, 1000)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Exploratory Data Analysis (EDA)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['sales'], kde=True)\n",
        "plt.title('Sales Distribution')\n",
        "plt.show()\n",
        "\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# 3. Data Preparation\n",
        "X = df[['base_price', 'demand', 'competitor_price', 'inventory_level', 'seasonality']]\n",
        "y = df['sales']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Linear Regression Model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "print(f'Linear Regression - MSE: {mse_lr}, R^2: {r2_lr}')\n",
        "\n",
        "# 5. XGBoost Model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "print(f'XGBoost - MSE: {mse_xgb}, R^2: {r2_xgb}')\n",
        "\n",
        "# 6. Visualization of Predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test.values, label='Actual Sales', marker='o')\n",
        "plt.plot(y_pred_lr, label='Predicted Sales (Linear Regression)', linestyle='--')\n",
        "plt.plot(y_pred_xgb, label='Predicted Sales (XGBoost)', linestyle='-.')\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eK5a2JV3OYc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.stats import skew\n",
        "\n",
        "# 1. Data Preparation & Exploration\n",
        "df = pd.read_csv('wayfair_pricing_data.csv')  # Placeholder file name\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Summary statistics\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(df.info())\n",
        "\n",
        "# Visualize data distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "df.hist(bins=30, figsize=(15, 10))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Handling Skewed Data\n",
        "# Check skewness\n",
        "skewness = df.skew()\n",
        "print(\"Skewness of features:\\n\", skewness)\n",
        "\n",
        "# Apply log transformation to skewed data\n",
        "for col in df.select_dtypes(include=[np.number]).columns:\n",
        "    if abs(skew(df[col])) > 0.75:\n",
        "        df[col] = np.log1p(df[col])\n",
        "\n",
        "# Visualize after transformation\n",
        "plt.figure(figsize=(10, 6))\n",
        "df.hist(bins=30, figsize=(15, 10))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Feature Selection & Splitting\n",
        "features = ['competitor_price', 'demand', 'inventory_level', 'seasonality']\n",
        "target = 'price'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train Shape:\", X_train.shape)\n",
        "print(\"Test Shape:\", X_test.shape)\n",
        "\n",
        "# 4. Model Training\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = rf.predict(X_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Model Performance\n",
        "print(\"Train R^2:\", r2_score(y_train, y_train_pred))\n",
        "print(\"Test R^2:\", r2_score(y_test, y_pred))\n",
        "\n",
        "# 5. Feature Importance Visualization\n",
        "importances = rf.feature_importances_\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=importances, y=features)\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n",
        "\n",
        "# 6. Error Analysis\n",
        "errors = y_test - y_pred\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(errors, kde=True)\n",
        "plt.title('Prediction Errors Distribution')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f3B1J4BISIFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Step 1: Generate Synthetic Data\n",
        "np.random.seed(42)\n",
        "\n",
        "# Creating skewed price data\n",
        "prices = np.random.exponential(scale=100, size=1000)\n",
        "\n",
        "# Additional features\n",
        "product_age = np.random.randint(1, 365, size=1000)  # Product age in days\n",
        "ratings = np.random.uniform(1, 5, size=1000)        # Customer ratings\n",
        "sales_volume = np.random.poisson(lam=20, size=1000) # Sales volume\n",
        "\n",
        "# Combine into a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Price': prices,\n",
        "    'Product_Age': product_age,\n",
        "    'Ratings': ratings,\n",
        "    'Sales_Volume': sales_volume\n",
        "})\n",
        "\n",
        "# Step 2: Exploratory Data Analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data['Price'], kde=True)\n",
        "plt.title('Original Price Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Check skewness\n",
        "print(f\"Skewness of Price: {skew(data['Price'])}\")\n",
        "\n",
        "# Step 3: Handling Skewed Data\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "data['Price_Transformed'] = pt.fit_transform(data[['Price']])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data['Price_Transformed'], kde=True)\n",
        "plt.title('Transformed Price Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Preparing Data for Modeling\n",
        "X = data[['Product_Age', 'Ratings', 'Sales_Volume']]\n",
        "y = data['Price_Transformed']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Building the Model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predictions and Evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Visualization of Predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "plt.xlabel('Actual Prices (Transformed)')\n",
        "plt.ylabel('Predicted Prices (Transformed)')\n",
        "plt.title('Actual vs Predicted Prices')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "liGuldMoSxlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Step 1: Generate Synthetic Data\n",
        "np.random.seed(42)\n",
        "data = pd.DataFrame({\n",
        "    'base_price': np.random.uniform(20, 100, 1000),\n",
        "    'demand': np.random.uniform(100, 1000, 1000),\n",
        "    'competitor_price': np.random.uniform(15, 105, 1000),\n",
        "    'inventory_levels': np.random.randint(50, 500, 1000),\n",
        "    'sales': np.random.randint(50, 1000, 1000)\n",
        "})\n",
        "\n",
        "# Step 2: Target Variable (Revenue)\n",
        "data['revenue'] = data['base_price'] * data['sales']\n",
        "\n",
        "# Step 3: Train Demand Prediction Model\n",
        "features = ['base_price', 'competitor_price', 'inventory_levels', 'sales']\n",
        "X = data[features]\n",
        "y = data['demand']\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Step 4: Price Optimization Function\n",
        "def optimize_price(base_price, competitor_price, inventory, sales):\n",
        "    def revenue_function(price):\n",
        "        predicted_demand = model.predict([[price, competitor_price, inventory, sales]])[0]\n",
        "        return -price * predicted_demand  # Negative for minimization\n",
        "\n",
        "    result = minimize(revenue_function, x0=base_price, bounds=[(10, 200)])\n",
        "    return result.x[0] if result.success else base_price\n",
        "\n",
        "# Step 5: Apply Optimization\n",
        "optimized_prices = []\n",
        "for i in range(len(data)):\n",
        "    opt_price = optimize_price(\n",
        "        data.loc[i, 'base_price'],\n",
        "        data.loc[i, 'competitor_price'],\n",
        "        data.loc[i, 'inventory_levels'],\n",
        "        data.loc[i, 'sales']\n",
        "    )\n",
        "    optimized_prices.append(opt_price)\n",
        "\n",
        "# Step 6: Compare Base vs Optimized Prices\n",
        "data['optimized_price'] = optimized_prices\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['base_price'][:50], label='Base Price', marker='o')\n",
        "plt.plot(data['optimized_price'][:50], label='Optimized Price', marker='x')\n",
        "plt.title('Base Price vs Optimized Price')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wb1uYgzAUK-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Synthetic Data Generation\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate customers, products, and purchase history\n",
        "customers = [f'Customer_{i}' for i in range(1, 11)]\n",
        "products = [f'Product_{i}' for i in range(1, 11)]\n",
        "categories = ['Furniture', 'Decor', 'Appliances', 'Outdoor', 'Lighting']\n",
        "\n",
        "# Purchase history\n",
        "purchase_history = pd.DataFrame({\n",
        "    'CustomerID': np.random.choice(customers, 50),\n",
        "    'ProductID': np.random.choice(products, 50),\n",
        "    'Rating': np.random.randint(1, 6, 50),\n",
        "    'Category': np.random.choice(categories, 50)\n",
        "})\n",
        "\n",
        "# Display sample data\n",
        "print(purchase_history.head())\n",
        "\n",
        "# 2. Collaborative Filtering (using Surprise library)\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(purchase_history[['CustomerID', 'ProductID', 'Rating']], reader)\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "algo = SVD()\n",
        "algo.fit(trainset)\n",
        "\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "# 3. Content-Based Filtering\n",
        "# Create a TF-IDF matrix for product categories\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(purchase_history['Category'])\n",
        "\n",
        "# Compute cosine similarity\n",
        "cos_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Recommend products based on similar categories\n",
        "idx = 0  # Example for the first product\n",
        "similar_indices = cos_sim[idx].argsort()[-4:][::-1]  # Top 3 similar\n",
        "print(\"\\nContent-Based Recommendations:\")\n",
        "print(purchase_history.iloc[similar_indices][['ProductID', 'Category']])\n",
        "\n",
        "# 4. Hybrid Recommendation (Combining both)\n",
        "def hybrid_recommendation(customer_id):\n",
        "    user_ratings = purchase_history[purchase_history['CustomerID'] == customer_id]\n",
        "    if user_ratings.empty:\n",
        "        return \"No purchase history available.\"\n",
        "\n",
        "    last_product = user_ratings.iloc[-1]['ProductID']\n",
        "    similar_indices = cos_sim[purchase_history[purchase_history['ProductID'] == last_product].index[0]].argsort()[-4:][::-1]\n",
        "\n",
        "    collaborative_preds = [(algo.predict(customer_id, prod_id).est, prod_id)\n",
        "                            for prod_id in purchase_history['ProductID'].unique()]\n",
        "    collaborative_preds.sort(reverse=True)\n",
        "\n",
        "    recommendations = set([prod for _, prod in collaborative_preds[:3]])\n",
        "    recommendations.update(purchase_history.iloc[similar_indices]['ProductID'].values[:3])\n",
        "\n",
        "    return list(recommendations)\n",
        "\n",
        "# Example for Customer_1\n",
        "print(\"\\nHybrid Recommendations for Customer_1:\")\n",
        "print(hybrid_recommendation('Customer_1'))\n"
      ],
      "metadata": {
        "id": "WCW8ZLlwZEQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Synthetic Data\n",
        "users = ['User1', 'User2', 'User3', 'User4', 'User5']\n",
        "products = ['Chair', 'Table', 'Lamp', 'Sofa', 'Bed']\n",
        "\n",
        "# User-Item Ratings\n",
        "ratings_data = {\n",
        "    'User': np.random.choice(users, 20),\n",
        "    'Product': np.random.choice(products, 20),\n",
        "    'Rating': np.random.randint(1, 6, 20)\n",
        "}\n",
        "ratings_df = pd.DataFrame(ratings_data)\n",
        "print(\"Ratings Data:\")\n",
        "print(ratings_df)\n",
        "\n",
        "# Content-Based Filtering (CBF) Data\n",
        "product_descriptions = {\n",
        "    'Chair': 'Comfortable wooden chair with cushion',\n",
        "    'Table': 'Large dining table with modern design',\n",
        "    'Lamp': 'LED lamp with adjustable brightness',\n",
        "    'Sofa': 'Cozy sofa with soft fabric',\n",
        "    'Bed': 'King size bed with memory foam mattress'\n",
        "}\n",
        "products_df = pd.DataFrame(list(product_descriptions.items()), columns=['Product', 'Description'])\n",
        "\n",
        "# Collaborative Filtering (CF)\n",
        "user_item_matrix = ratings_df.pivot_table(index='User', columns='Product', values='Rating').fillna(0)\n",
        "user_similarity = cosine_similarity(user_item_matrix)\n",
        "user_similarity_df = pd.DataFrame(user_similarity, index=users, columns=users)\n",
        "print(\"\\nUser Similarity Matrix:\")\n",
        "print(user_similarity_df)\n",
        "\n",
        "# Content-Based Filtering (CBF)\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(products_df['Description'])\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "cbf_recommendations = {}\n",
        "for idx, product in enumerate(products):\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:3]\n",
        "    recommended_products = [products[i[0]] for i in sim_scores]\n",
        "    cbf_recommendations[product] = recommended_products\n",
        "\n",
        "print(\"\\nContent-Based Recommendations:\")\n",
        "for product, recs in cbf_recommendations.items():\n",
        "    print(f\"{product}: {recs}\")\n",
        "\n",
        "# Hybrid Recommendation System\n",
        "hybrid_recommendations = {}\n",
        "for user in users:\n",
        "    cf_scores = user_item_matrix.loc[user].sort_values(ascending=False)\n",
        "    cbf_scores = pd.Series(np.random.rand(len(products)), index=products)\n",
        "    hybrid_scores = cf_scores.add(cbf_scores, fill_value=0).sort_values(ascending=False)\n",
        "    hybrid_recommendations[user] = hybrid_scores.head(2).index.tolist()\n",
        "\n",
        "print(\"\\nHybrid Recommendations:\")\n",
        "for user, recs in hybrid_recommendations.items():\n",
        "    print(f\"{user}: {recs}\")\n"
      ],
      "metadata": {
        "id": "oiGNDHhOZ32D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# 1. Generate Synthetic Data\n",
        "np.random.seed(42)\n",
        "n_customers = 1000\n",
        "data = {\n",
        "    'CustomerID': np.arange(1, n_customers + 1),\n",
        "    'Age': np.random.randint(18, 70, size=n_customers),\n",
        "    'Tenure': np.random.randint(1, 60, size=n_customers),\n",
        "    'TotalSpend': np.random.uniform(100, 10000, size=n_customers),\n",
        "    'PurchaseFrequency': np.random.poisson(2, size=n_customers),\n",
        "    'LastPurchaseDays': np.random.randint(0, 365, size=n_customers),\n",
        "    'IsChurn': np.random.choice([0, 1], size=n_customers, p=[0.8, 0.2])\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Exploratory Data Analysis\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.countplot(x='IsChurn', data=df)\n",
        "plt.title('Churn Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Correlation Matrix\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Feature Correlation')\n",
        "plt.show()\n",
        "\n",
        "# 3. Data Preprocessing\n",
        "X = df[['Age', 'Tenure', 'TotalSpend', 'PurchaseFrequency', 'LastPurchaseDays']]\n",
        "y = df['IsChurn']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Model Building\n",
        "# Logistic Regression\n",
        "log_model = LogisticRegression()\n",
        "log_model.fit(X_train, y_train)\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# 5. Evaluation Metrics\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\n",
        "\n",
        "print(\"\\nRandom Forest Results:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Random Forest (AUC = {:.2f})'.format(roc_auc_score(y_test, y_pred_rf)))\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "An5C2qmfkGUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Create synthetic data\n",
        "data = {\n",
        "    'query': [\n",
        "        'best laptop for programming',\n",
        "        'cheap smartphones with good camera',\n",
        "        'top running shoes for marathon',\n",
        "        'best coffee machines for home',\n",
        "        'affordable noise cancelling headphones'\n",
        "    ],\n",
        "    'document': [\n",
        "        'laptop with i7 processor and 16GB RAM, ideal for programming',\n",
        "        'smartphone under $300 with 48MP camera',\n",
        "        'running shoes designed for marathon with breathable material',\n",
        "        'coffee machine with grinder and milk frother for home use',\n",
        "        'budget headphones with active noise cancellation'\n",
        "    ],\n",
        "    'clicked': [1, 1, 1, 1, 0]  # 1 = Clicked (relevant), 0 = Not clicked\n",
        "}\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "df = pd.DataFrame(data)\n",
        "df['combined'] = df['query'] + \" \" + df['document']\n",
        "\n",
        "# Step 3: Feature Extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['combined'])\n",
        "y = df['clicked']\n",
        "\n",
        "# Step 4: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Model Training\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predictions and Evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "sLR2QS11oCT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import networkx as nx\n",
        "\n",
        "# Simulated web pages\n",
        "data = {\n",
        "    'url': ['page1.com', 'page2.com', 'page3.com'],\n",
        "    'content': [\n",
        "        'Machine learning is the future of technology.',\n",
        "        'Deep learning drives many AI applications.',\n",
        "        'Data science includes machine learning and AI.'\n",
        "    ],\n",
        "    'links': [['page2.com'], ['page1.com', 'page3.com'], ['page1.com']]\n",
        "}\n",
        "\n",
        "# Creating a DataFrame\n",
        "web_pages = pd.DataFrame(data)\n",
        "\n",
        "# Indexing using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(web_pages['content'])\n",
        "\n",
        "# PageRank Calculation\n",
        "graph = nx.DiGraph()\n",
        "for i, links in enumerate(web_pages['links']):\n",
        "    for link in links:\n",
        "        graph.add_edge(web_pages['url'][i], link)\n",
        "\n",
        "pagerank_scores = nx.pagerank(graph)\n",
        "\n",
        "# Query Processing\n",
        "def search(query):\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    scores = (tfidf_matrix * query_vec.T).toarray().flatten()\n",
        "    results = pd.DataFrame({\n",
        "        'url': web_pages['url'],\n",
        "        'relevance_score': scores,\n",
        "        'pagerank': [pagerank_scores[url] for url in web_pages['url']]\n",
        "    })\n",
        "    # Combining relevance score and PageRank\n",
        "    results['final_score'] = results['relevance_score'] * 0.7 + results['pagerank'] * 0.3\n",
        "    return results.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "# Example Query\n",
        "results = search('machine learning')\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "7yJOJf3gpHz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Step 1: Generate Synthetic Data (Simulating Product Features)\n",
        "d = 64  # Dimension of the feature vectors (like embedding size)\n",
        "n = 10000  # Number of data points (products)\n",
        "\n",
        "# Random data representing product embeddings\n",
        "data = np.random.random((n, d)).astype('float32')\n",
        "\n",
        "# Step 2: Build the FAISS Index\n",
        "index = faiss.IndexFlatL2(d)  # L2 distance (Euclidean distance) for similarity\n",
        "index.add(data)  # Add the product data to the index\n",
        "\n",
        "# Step 3: Query Similar Items\n",
        "query_vector = np.random.random((1, d)).astype('float32')  # A random product to search for\n",
        "\n",
        "# Perform the search (returns distances and indices of the closest items)\n",
        "distances, indices = index.search(query_vector, k=5)  # Find top 5 similar items\n",
        "\n",
        "# Step 4: Display the Results\n",
        "print(\"Query Vector:\", query_vector)\n",
        "print(\"Indices of Similar Items:\", indices)\n",
        "print(\"Distances to Similar Items:\", distances)"
      ],
      "metadata": {
        "id": "iu6f_bWDtAIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Simulate Synthetic Data\n",
        "np.random.seed(42)\n",
        "X, _ = make_blobs(n_samples=100, n_features=128, centers=5)  # Simulating image features\n",
        "X = StandardScaler().fit_transform(X)  # Normalize features\n",
        "\n",
        "# Step 2: Build FAISS Index\n",
        "d = X.shape[1]  # Feature dimension\n",
        "index = faiss.IndexFlatL2(d)  # L2 distance index\n",
        "index.add(X)  # Add data to index\n",
        "\n",
        "# Step 3: Query Example\n",
        "query_vector = X[0].reshape(1, -1)  # Using the first vector as a query\n",
        "k = 5  # Retrieve top 5 similar items\n",
        "distances, indices = index.search(query_vector, k)\n",
        "\n",
        "# Step 4: Display Results\n",
        "print(\"Query Vector:\", query_vector)\n",
        "print(\"Top 5 Similar Items (Indices):\", indices)\n",
        "print(\"Distances:\", distances)\n",
        "\n",
        "# Visualization\n",
        "plt.scatter(X[:, 0], X[:, 1], color='gray')\n",
        "plt.scatter(query_vector[:, 0], query_vector[:, 1], color='red', label='Query')\n",
        "plt.scatter(X[indices[0], 0], X[indices[0], 1], color='blue', label='Similar Items')\n",
        "plt.legend()\n",
        "plt.title('Visual Search Using FAISS')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H4erNITyuBy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import faiss\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Data Preparation (Synthetic Example)\n",
        "product_ids = [f'Product_{i}' for i in range(10)]\n",
        "product_images = [np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8) for _ in range(10)]\n",
        "\n",
        "# 2. Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 3. Feature Extraction Using Pretrained ResNet\n",
        "model = models.resnet50(pretrained=True)\n",
        "model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove final classification layer\n",
        "model.eval()\n",
        "\n",
        "# Extract features\n",
        "def extract_features(images):\n",
        "    features = []\n",
        "    for img in images:\n",
        "        img_tensor = transform(img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            feature = model(img_tensor).squeeze().numpy()\n",
        "        features.append(feature)\n",
        "    return np.array(features)\n",
        "\n",
        "features = extract_features(product_images)\n",
        "\n",
        "# 4. Similarity Search with FAISS\n",
        "feature_dim = features.shape[1]\n",
        "index = faiss.IndexFlatL2(feature_dim)\n",
        "index.add(features)\n",
        "\n",
        "# 5. Query Image\n",
        "query_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
        "query_feature = extract_features([query_image])\n",
        "\n",
        "# 6. Retrieve Similar Products\n",
        "distances, indices = index.search(query_feature, k=3)\n",
        "\n",
        "# 7. Display Results\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(query_image)\n",
        "plt.title('Query Image')\n",
        "plt.axis('off')\n",
        "\n",
        "for i, idx in enumerate(indices[0]):\n",
        "    plt.subplot(1, 4, i + 2)\n",
        "    plt.imshow(product_images[idx])\n",
        "    plt.title(f'Similar {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ox3YVBvHw1R0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}